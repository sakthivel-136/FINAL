# -*- coding: utf-8 -*-
"""mic_chat

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Ei3nfgH_cdVVbbmd-_EQurgt7s4rvQs
"""

import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode
import av
import queue
import threading
import time
import tempfile
from gtts import gTTS
from scipy.io.wavfile import write
import numpy as np
import pandas as pd
import os
import pickle
import speech_recognition as sr
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from streamlit_extras.add_vertical_space import add_vertical_space

# Constants
CSV_FILE = "kcet.csv"
VECTOR_FILE = "vectorized.pkl"
THRESHOLD = 0.8
WAKE_WORD = "hey kcet"

# Load or vectorize CSV data
def load_or_vectorize():
    if os.path.exists(VECTOR_FILE):
        with open(VECTOR_FILE, "rb") as f:
            vectorizer, vectors, df = pickle.load(f)
    else:
        df = pd.read_csv(CSV_FILE)
        df['Question'] = df['Question'].str.strip().str.lower()
        vectorizer = TfidfVectorizer()
        vectors = vectorizer.fit_transform(df['Question'])
        with open(VECTOR_FILE, "wb") as f:
            pickle.dump((vectorizer, vectors, df), f)
    return vectorizer, vectors, df

vectorizer, vectors, df = load_or_vectorize()
audio_queue = queue.Queue()

st.set_page_config(page_title="üéôÔ∏è KCET Voice Assistant", layout="centered")
st.markdown("""
    <style>
    .bot-bubble {
        background-color: #e0f7fa;
        color: #006064;
        padding: 1em;
        border-radius: 12px;
        margin-bottom: 10px;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .user-bubble {
        background-color: #fce4ec;
        color: #880e4f;
        padding: 1em;
        border-radius: 12px;
        margin-bottom: 10px;
        text-align: right;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .typing {
        font-style: italic;
        color: #9e9e9e;
        animation: pulse 1s infinite;
    }
    @keyframes pulse {
        0% { opacity: 0.3; }
        50% { opacity: 1; }
        100% { opacity: 0.3; }
    }
    </style>
""", unsafe_allow_html=True)

st.title("üéôÔ∏è KCET Voice Assistant")
status = st.empty()
transcript_placeholder = st.empty()
bot_response = st.empty()
history_placeholder = st.container()

chat_history = []

class AudioProcessor:
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        audio = frame.to_ndarray()
        audio_queue.put_nowait(audio)
        return frame

def listen_and_process():
    recognizer = sr.Recognizer()
    status.info("üöÄ Background listener started...")

    while True:
        if audio_queue.qsize() > 150:
            status.info("üé§ Listening for Wake Word...")
            audio_data = np.concatenate(list(audio_queue.queue), axis=0).astype(np.int16)
            audio_queue.queue.clear()

            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_wav:
                write(temp_wav.name, 16000, audio_data)
                with sr.AudioFile(temp_wav.name) as source:
                    try:
                        audio = recognizer.record(source)
                        query = recognizer.recognize_google(audio).lower()
                        print(f"üéß Transcribed: {query}")
                    except Exception as e:
                        status.error(f"Speech error: {e}")
                        continue

            if WAKE_WORD in query:
                status.success("‚úÖ Wake word detected!")
                transcript_placeholder.markdown("üß† Waiting for your question...")
                time.sleep(1)
                while audio_queue.qsize() < 150:
                    time.sleep(0.2)

                audio_data = np.concatenate(list(audio_queue.queue), axis=0).astype(np.int16)
                audio_queue.queue.clear()
                with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_wav:
                    write(temp_wav.name, 16000, audio_data)
                    with sr.AudioFile(temp_wav.name) as source:
                        try:
                            audio = recognizer.record(source)
                            query = recognizer.recognize_google(audio).lower()
                            live = ""
                            for word in query.split():
                                live += word + " "
                                transcript_placeholder.markdown(f"<div class='user-bubble'>üë§ {live.strip()}</div>", unsafe_allow_html=True)
                                time.sleep(0.2)
                        except Exception as e:
                            status.error("‚ùå Failed to process your speech.")
                            continue

                query_vector = vectorizer.transform([query])
                similarity = cosine_similarity(query_vector, vectors)
                max_sim = similarity.max()
                max_index = similarity.argmax()

                if max_sim >= THRESHOLD:
                    answer = df.iloc[max_index]['Answer']
                else:
                    answer = "ü§ñ I couldn't understand that. Please ask again."

                chat_history.append((query, answer))
                bot_response.markdown(f"<div class='bot-bubble typing'>ü§ñ Thinking...</div>", unsafe_allow_html=True)
                time.sleep(1)
                bot_response.markdown(f"<div class='bot-bubble'>ü§ñ {answer}</div>", unsafe_allow_html=True)

                tts = gTTS(answer)
                tts_fp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
                tts.save(tts_fp.name)
                st.audio(tts_fp.name, format="audio/mp3")
                status.empty()

                with history_placeholder:
                    st.markdown("## üìú Chat History")
                    for user, bot in reversed(chat_history):
                        st.markdown(f"<div class='user-bubble'>üë§ {user}</div>", unsafe_allow_html=True)
                        st.markdown(f"<div class='bot-bubble'>ü§ñ {bot}</div>", unsafe_allow_html=True)
                        add_vertical_space(1)

webrtc_streamer(
    key="voice",
    mode=WebRtcMode.SENDONLY,
    audio_processor_factory=AudioProcessor,
    media_stream_constraints={"audio": True, "video": False},
)

threading.Thread(target=listen_and_process, daemon=True).start()
