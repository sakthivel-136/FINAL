# -*- coding: utf-8 -*-
"""mic_chat

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Ei3nfgH_cdVVbbmd-_EQurgt7s4rvQs
"""
# -*- coding: utf-8 -*-
"""mic_chat
Automatically generated by Colab.
"""
import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode
import av
import queue
import threading
import time
import tempfile
from gtts import gTTS
from scipy.io.wavfile import write
import numpy as np
import pandas as pd
import os
import pickle
import speech_recognition as sr
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import base64
import uuid

st.set_page_config(page_title="ðŸŽ™ï¸ KCET Voice Assistant", layout="centered")

CSV_FILE = "kcet.csv"
VECTOR_FILE = "vectorized.pkl"
THRESHOLD = 0.8
WAKE_WORD = "hey kcet"

@st.cache_resource
def load_or_vectorize():
    if os.path.exists(VECTOR_FILE):
        with open(VECTOR_FILE, "rb") as f:
            vectorizer, vectors, df = pickle.load(f)
    else:
        if not os.path.exists(CSV_FILE):
            st.error(f"Error: '{CSV_FILE}' not found.")
            st.stop()
        df = pd.read_csv(CSV_FILE)
        if 'Question' not in df.columns or 'Answer' not in df.columns:
            st.error("CSV must contain 'Question' and 'Answer' columns.")
            st.stop()
        df['Question'] = df['Question'].astype(str).str.strip().str.lower()
        vectorizer = TfidfVectorizer()
        vectors = vectorizer.fit_transform(df['Question'])
        with open(VECTOR_FILE, "wb") as f:
            pickle.dump((vectorizer, vectors, df), f)
    return vectorizer, vectors, df

vectorizer, vectors, df = load_or_vectorize()

if "audio_queue" not in st.session_state:
    st.session_state["audio_queue"] = queue.Queue()
if "listening_active_event" not in st.session_state:
    st.session_state["listening_active_event"] = threading.Event()
if "listening_thread" not in st.session_state:
    st.session_state["listening_thread"] = None
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []
if "debug_message" not in st.session_state:
    st.session_state["debug_message"] = "Initializing..."
if "show_history" not in st.session_state:
    st.session_state["show_history"] = False

st.title("ðŸŽ™ï¸ KCET Voice Assistant")
status = st.empty()
transcript_placeholder = st.empty()
bot_response = st.empty()
debug_placeholder = st.empty()

class AudioProcessor:
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        if st.session_state["listening_active_event"].is_set():
            audio = frame.to_ndarray(format="s16", layout="mono")
            try:
                st.session_state["audio_queue"].put_nowait(audio)
            except queue.Full:
                st.session_state["debug_message"] = "Audio queue full."
        return frame

def listen_and_process_thread(audio_q: queue.Queue, listening_event: threading.Event):
    recognizer = sr.Recognizer()
    recognizer.energy_threshold = 700
    recognizer.dynamic_energy_threshold = True
    recognizer.pause_threshold = 0.8
    temp_wav_path = None

    while listening_event.is_set():
        try:
            if audio_q.qsize() > 200:
                audio_data_list = []
                while not audio_q.empty():
                    try:
                        audio_data_list.append(audio_q.get_nowait())
                    except queue.Empty:
                        break
                if not audio_data_list:
                    time.sleep(0.01)
                    continue
                combined_audio_data = np.concatenate(audio_data_list, axis=0).astype(np.int16)
                with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_wav:
                    write(temp_wav.name, 16000, combined_audio_data)
                    temp_wav_path = temp_wav.name
                with sr.AudioFile(temp_wav_path) as source:
                    try:
                        audio = recognizer.record(source)
                        query = recognizer.recognize_google(audio, language="en-IN").lower()
                        if WAKE_WORD in query:
                            processed_query = query.replace(WAKE_WORD, "").strip()
                            if processed_query:
                                query_vector = vectorizer.transform([processed_query])
                                similarity = cosine_similarity(query_vector, vectors)
                                max_sim = similarity.max()
                                max_index = similarity.argmax()
                                answer = df.iloc[max_index]['Answer'] if max_sim >= THRESHOLD else "ðŸ¤– I couldn't understand that."
                                st.session_state["new_query"] = processed_query
                                st.session_state["new_answer"] = answer
                                st.rerun()
                        else:
                            st.session_state["debug_message"] = f"'{query}' (no wake word detected)"
                    except Exception as e:
                        st.session_state["debug_message"] = f"SR error: {e}"
                    finally:
                        if temp_wav_path and os.path.exists(temp_wav_path):
                            os.unlink(temp_wav_path)
            else:
                if audio_q.qsize() < 10:
                    st.session_state["debug_message"] = f"Waiting... ({audio_q.qsize()} frames)"
                time.sleep(0.05)
        except Exception as e:
            st.session_state["debug_message"] = f"Fatal thread error: {e}"
            listening_event.clear()
            break

webrtc_ctx = webrtc_streamer(
    key="voice",
    mode=WebRtcMode.SENDONLY,
    audio_processor_factory=AudioProcessor,
    media_stream_constraints={"audio": True, "video": False},
    rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
)

if webrtc_ctx.state.playing and not st.session_state["listening_active_event"].is_set():
    st.session_state["listening_active_event"].set()
    if st.session_state["listening_thread"] is None or not st.session_state["listening_thread"].is_alive():
        st.session_state["listening_thread"] = threading.Thread(
            target=listen_and_process_thread,
            args=(st.session_state["audio_queue"], st.session_state["listening_active_event"]),
            daemon=True
        )
        st.session_state["listening_thread"].start()
    status.write("ðŸ‘‚ Listening for 'Hey KCET'...")
else:
    status.write("ðŸ”´ Not listening.")

debug_placeholder.info(st.session_state["debug_message"])

with st.form("manual_input_form", clear_on_submit=True):
    user_query_manual = st.text_input("ðŸ’¬ Type your question:", key="manual_text_input")
    submitted = st.form_submit_button("Submit")
    if submitted and user_query_manual.strip():
        query_vector = vectorizer.transform([user_query_manual.strip().lower()])
        similarity = cosine_similarity(query_vector, vectors)
        max_sim = similarity.max()
        max_index = similarity.argmax()
        answer = df.iloc[max_index]['Answer'] if max_sim >= THRESHOLD else "ðŸ¤– I couldn't understand that."
        st.session_state["new_query"] = user_query_manual.strip().lower()
        st.session_state["new_answer"] = answer
        st.rerun()

if "new_query" in st.session_state and "new_answer" in st.session_state:
    user_q = st.session_state.pop("new_query")
    bot_a = st.session_state.pop("new_answer")
    st.session_state["chat_history"].append((user_q, bot_a))
    transcript_placeholder.markdown(f"<div class='user-bubble'>ðŸ‘¤ **You:** {user_q}</div>", unsafe_allow_html=True)
    bot_response.markdown(f"<div class='bot-bubble'>ðŸ¤– **KCET Bot:** {bot_a}</div>", unsafe_allow_html=True)

    try:
        tts = gTTS(bot_a, lang='en')
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tts_fp:
            tts.save(tts_fp.name)
            tts_file_path = tts_fp.name
        audio_bytes = open(tts_file_path, "rb").read()
        audio_b64 = base64.b64encode(audio_bytes).decode()
        audio_id = f"ttsAudio_{uuid.uuid4().hex}"
        audio_html = f'''
        <audio id="{audio_id}" autoplay hidden>
            <source src="data:audio/mp3;base64,{audio_b64}" type="audio/mp3">
        </audio>
        <script>
        var audio = document.getElementById("{audio_id}");
        if (audio) {{
            audio.play().catch(err => console.log("Autoplay failed:", err));
        }}
        </script>
        '''
        st.markdown(audio_html, unsafe_allow_html=True)
        os.unlink(tts_file_path)
    except Exception as e:
        st.error(f"Audio error: {e}")

st.markdown("---")

if st.button("Show History" if not st.session_state["show_history"] else "Hide History"):
    st.session_state["show_history"] = not st.session_state["show_history"]

if st.session_state["show_history"]:
    if not st.session_state["chat_history"]:
        st.info("No chat history yet.")
    else:
        for user_hist, bot_hist in reversed(st.session_state["chat_history"]):
            st.markdown(f"<div class='user-bubble'>ðŸ‘¤ **You:** {user_hist}</div>", unsafe_allow_html=True)
            st.markdown(f"<div class='bot-bubble'>ðŸ¤– **KCET Bot:** {bot_hist}</div>", unsafe_allow_html=True)
