# -*- coding: utf-8 -*-
"""mic_chat

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Ei3nfgH_cdVVbbmd-_EQurgt7s4rvQs
"""

import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode
import av
import queue
import threading
import time
import tempfile
from gtts import gTTS
from scipy.io.wavfile import write
import numpy as np
import pandas as pd
import os
import pickle
import speech_recognition as sr
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Constants
CSV_FILE = "kcet.csv"
VECTOR_FILE = "vectorized.pkl"
THRESHOLD = 0.8
WAKE_WORD = "hey kcet"

# Load or vectorize CSV data
def load_or_vectorize():
    if os.path.exists(VECTOR_FILE):
        with open(VECTOR_FILE, "rb") as f:
            vectorizer, vectors, df = pickle.load(f)
    else:
        df = pd.read_csv(CSV_FILE)
        df['Question'] = df['Question'].str.strip().str.lower()
        vectorizer = TfidfVectorizer()
        vectors = vectorizer.fit_transform(df['Question'])
        with open(VECTOR_FILE, "wb") as f:
            pickle.dump((vectorizer, vectors, df), f)
    return vectorizer, vectors, df

vectorizer, vectors, df = load_or_vectorize()

# --- Initialize session state variables at the very top ---
# This ensures they exist before any thread or part of the script tries to access them.
if "audio_queue" not in st.session_state:
    st.session_state["audio_queue"] = queue.Queue()
if "listening_active_event" not in st.session_state: # Renamed for clarity, it's a threading.Event
    st.session_state["listening_active_event"] = threading.Event()
if "listening_thread" not in st.session_state:
    st.session_state["listening_thread"] = None # To hold the thread object
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

st.set_page_config(page_title="üéôÔ∏è KCET Voice Assistant", layout="centered")
st.markdown("""
    <style>
    .bot-bubble {
        background-color: #e0f7fa;
        color: #006064;
        padding: 1em;
        border-radius: 12px;
        margin-bottom: 10px;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .user-bubble {
        background-color: #fce4ec;
        color: #880e4f;
        padding: 1em;
        border-radius: 12px;
        margin-bottom: 10px;
        text-align: right;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .typing {
        font-style: italic;
        color: #9e9e9e;
        animation: pulse 1s infinite;
    }
    @keyframes pulse {
        0% { opacity: 0.3; }
        50% { opacity: 1; }
        100% { opacity: 0.3; }
    }
    </style>
""", unsafe_allow_html=True)

st.title("üéôÔ∏è KCET Voice Assistant")
status = st.empty()
transcript_placeholder = st.empty()
bot_response = st.empty()
history_placeholder = st.container()
manual_input_placeholder = st.empty()

class AudioProcessor:
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        # Check the event flag before putting audio in queue
        if st.session_state["listening_active_event"].is_set():
            audio = frame.to_ndarray()
            try:
                st.session_state["audio_queue"].put_nowait(audio)
            except queue.Full:
                # Optionally, log or handle a full queue
                pass
        return frame

def process_query_and_update_state(query, answer):
    # This function is called from the thread, but modifies session_state.
    # We use a dummy key to trigger a Streamlit rerun, and let the main script
    # handle the actual display and TTS.
    st.session_state["new_query"] = query
    st.session_state["new_answer"] = answer

def listen_and_process_thread(audio_q: queue.Queue, listening_event: threading.Event):
    recognizer = sr.Recognizer()

    while listening_event.is_set(): # Check the passed event flag
        try:
            # Check for sufficient audio data in the queue
            if audio_q.qsize() > 150: # Adjust this threshold based on your desired chunk size
                audio_data_list = []
                while not audio_q.empty():
                    try:
                        audio_data_list.append(audio_q.get_nowait())
                    except queue.Empty:
                        break # Break if queue becomes empty while getting

                if not audio_data_list: # If list is empty after trying to get
                    time.sleep(0.01) # Small sleep to prevent busy-waiting
                    continue

                audio_data = np.concatenate(audio_data_list, axis=0).astype(np.int16)

                with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_wav:
                    write(temp_wav.name, 16000, audio_data)
                    # No need for a nested `with sr.AudioFile`, just use the path
                    with sr.AudioFile(temp_wav.name) as source:
                        try:
                            # Adjust this duration based on how long you expect a phrase after wake word
                            # Or capture a longer segment and then re-process for the command
                            audio = recognizer.record(source, duration=5) # Record up to 5 seconds
                            query = recognizer.recognize_google(audio).lower()
                            print(f"Recognized (in thread): {query}") # For debugging

                            if WAKE_WORD in query:
                                # Trim the wake word
                                processed_query = query.replace(WAKE_WORD, "").strip()
                                if processed_query:
                                    # Call a function that can trigger a Streamlit rerun
                                    # by modifying session_state
                                    # Note: Streamlit usually recommends passing a callback
                                    # function reference to the thread that then
                                    # modifies session_state on the main loop.
                                    # Directly modifying st.session_state from a thread
                                    # can sometimes lead to context issues, but for
                                    # simple flag/value setting to trigger a rerun, it often works.
                                    query_vector = vectorizer.transform([processed_query])
                                    similarity = cosine_similarity(query_vector, vectors)
                                    max_sim = similarity.max()
                                    max_index = similarity.argmax()

                                    if max_sim >= THRESHOLD:
                                        answer = df.iloc[max_index]['Answer']
                                    else:
                                        answer = "ü§ñ I couldn't understand that. Please ask again."

                                    # Set the session state variables to trigger a redraw
                                    st.session_state["new_query"] = processed_query
                                    st.session_state["new_answer"] = answer
                                else:
                                    # Wake word detected, but no command followed immediately
                                    pass # Or provide a "waiting for command" prompt

                                # Clear the queue after processing a command to avoid old audio
                                while not audio_q.empty():
                                    try:
                                        audio_q.get_nowait()
                                    except queue.Empty:
                                        pass # Should not happen here
                        except sr.UnknownValueError:
                            # Speech was unintelligible, ignore
                            pass
                        except sr.RequestError as e:
                            st.warning(f"Speech recognition service error (in thread): {e}")
                        except Exception as e:
                            st.error(f"An unexpected error occurred during audio processing in thread: {e}")
                        finally:
                            if os.path.exists(temp_wav.name):
                                os.unlink(temp_wav.name) # Ensure cleanup

            else:
                time.sleep(0.01) # Small sleep to prevent busy-waiting if queue is small

        except Exception as e:
            # Catch any high-level exceptions in the thread loop to prevent it from crashing silently
            st.error(f"Fatal error in listen_and_process_thread: {e}")
            break # Exit the loop if a fatal error occurs

# --- Streamlit UI and Thread Management ---

webrtc_ctx = webrtc_streamer(
    key="voice",
    mode=WebRtcMode.SENDONLY,
    audio_processor_factory=AudioProcessor,
    media_stream_constraints={"audio": True, "video": False},
    rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
)

# Start/Stop the listener thread based on WebRTC status
if webrtc_ctx.state.playing and not st.session_state["listening_active_event"].is_set():
    st.session_state["listening_active_event"].set() # Set the flag to true
    if st.session_state["listening_thread"] is None or not st.session_state["listening_thread"].is_alive():
        # Only start a new thread if one doesn't exist or isn't alive
        st.session_state["listening_thread"] = threading.Thread(
            target=listen_and_process_thread,
            args=(st.session_state["audio_queue"], st.session_state["listening_active_event"]),
            daemon=True
        )
        st.session_state["listening_thread"].start()
    status.write("üëÇ Listening for 'Hey KCET'...")
elif not webrtc_ctx.state.playing and st.session_state["listening_active_event"].is_set():
    st.session_state["listening_active_event"].clear() # Set the flag to false
    # Give the thread a moment to recognize the signal and exit
    if st.session_state["listening_thread"] and st.session_state["listening_thread"].is_alive():
        # Wait a short while for the thread to exit cleanly.
        # This join is non-blocking because of the timeout.
        st.session_state["listening_thread"].join(timeout=1)
    st.session_state["listening_thread"] = None # Reset the thread object

    # Clear the queue when connection stops to prevent stale data
    while not st.session_state["audio_queue"].empty():
        try:
            st.session_state["audio_queue"].get_nowait()
        except queue.Empty:
            pass
    status.write("üî¥ Not listening. Click 'Start' to activate.")
elif not webrtc_ctx.state.playing and not st.session_state["listening_active_event"].is_set():
    # Initial state or when stopped and already clear
    status.write("üî¥ Not listening. Click 'Start' to activate.")


# Manual input box
with manual_input_placeholder.form("manual_input_form"):
    user_query = st.text_input("üí¨ Type your question if you prefer not to speak:", "")
    submitted = st.form_submit_button("Submit")
    if submitted and user_query.strip():
        # Process manual query directly in the main thread
        query_vector = vectorizer.transform([user_query.strip().lower()])
        similarity = cosine_similarity(query_vector, vectors)
        max_sim = similarity.max()
        max_index = similarity.argmax()

        if max_sim >= THRESHOLD:
            answer = df.iloc[max_index]['Answer']
        else:
            answer = "ü§ñ I couldn't understand that. Please ask again."

        st.session_state["new_query"] = user_query.strip().lower()
        st.session_state["new_answer"] = answer

# Display chat history and responses
# This part runs in the main Streamlit thread on every rerun
if "new_query" in st.session_state and "new_answer" in st.session_state:
    user = st.session_state.pop("new_query")
    answer = st.session_state.pop("new_answer")

    st.session_state["chat_history"].append((user, answer))

    # Display current query and response
    transcript_placeholder.markdown(f"<div class='user-bubble'>üë§ {user}</div>", unsafe_allow_html=True)
    bot_response.markdown(f"<div class='bot-bubble typing'>ü§ñ Thinking...</div>", unsafe_allow_html=True)
    time.sleep(1) # Simulate thinking time
    bot_response.markdown(f"<div class='bot-bubble'>ü§ñ {answer}</div>", unsafe_allow_html=True)

    # Text-to-speech for the answer
    try:
        tts = gTTS(answer)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tts_fp:
            tts.save(tts_fp.name)
            tts_file_path = tts_fp.name # Store path before closing for unlink
        st.audio(tts_file_path, format="audio/mp3")
        os.unlink(tts_file_path) # Clean up the audio file
    except Exception as e:
        st.error(f"Error generating or playing audio: {e}")

# Display full chat history
with history_placeholder:
    st.markdown("## üìú Chat History")
    for user_hist, bot_hist in reversed(st.session_state["chat_history"]):
        st.markdown(f"<div class='user-bubble'>üë§ {user_hist}</div>", unsafe_allow_html=True)
        st.markdown(f"<div class='bot-bubble'>ü§ñ {bot_hist}</div>", unsafe_allow_html=True)
        st.markdown("<br>", unsafe_allow_html=True)
