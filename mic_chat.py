# -*- coding: utf-8 -*-
"""mic_chat

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Ei3nfgH_cdVVbbmd-_EQurgt7s4rvQs
"""


import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode
# Import AudioProcessorBase and ClientSettings from sub_modules
from streamlit_webrtc.sub_modules import AudioProcessorBase 
import numpy as np
import threading
import queue
import speech_recognition as sr
import os
from dotenv import load_dotenv
import time # Import time for sleep

# Load environment variables from .env file (if you have one)
load_dotenv()

# --- Configuration ---
WAKE_WORD = "hey kcet"
SPEAKER_NAME = "kcet" # Replace with your AI assistant's name
THRESHOLD = 0.75 # Similarity threshold for wake word (adjust as needed)
SAMPLE_RATE = 16000  # Sample rate for audio (common for speech)


# --- Session State Initialization ---
# Ensure all session state variables are initialized
if "listening_active" not in st.session_state:
    st.session_state["listening_active"] = False
if "new_query" not in st.session_state:
    st.session_state["new_query"] = None
if "new_answer" not in st.session_state:
    st.session_state["new_answer"] = None
if "debug_message" not in st.session_state:
    st.session_state["debug_message"] = "Initializing..."
if "audio_queue" not in st.session_state: # Use st.session_state to store the queue
    st.session_state["audio_queue"] = queue.Queue()
if "listen_event" not in st.session_state: # Use st.session_state for the event
    st.session_state["listen_event"] = threading.Event()
if "listen_thread_started" not in st.session_state:
    st.session_state["listen_thread_started"] = False


# --- Speech Recognition and Processing Thread ---
def listen_and_process_thread(listening_active_event: threading.Event, audio_q: queue.Queue):
    recognizer = sr.Recognizer()
    st.session_state["debug_message"] = "Thread started, awaiting activation."

    while True:
        # This will block the thread until the event is set (Streamlit button clicked)
        listening_active_event.wait()

        # If the event is cleared while waiting, loop back and wait again
        if not listening_active_event.is_set():
            continue

        st.session_state["debug_message"] = "üëÇ Listening for '" + WAKE_WORD + "'"
        # print("üëÇ Listening for '" + WAKE_WORD + "' (in thread)") # For terminal debugging

        audio_buffer_list = [] # List to accumulate numpy arrays of audio frames

        try:
            # Continue processing as long as the event is set
            while listening_active_event.is_set():
                if not audio_q.empty():
                    frame_array = audio_q.get() # Get numpy array from queue
                    audio_buffer_list.append(frame_array)

                    # Process accumulated audio after a certain buffer size (e.g., 2-3 seconds of audio)
                    # Let's check based on the total number of samples collected
                    total_samples = sum(len(arr) for arr in audio_buffer_list)

                    if total_samples >= SAMPLE_RATE * 2: # ~2 seconds of audio at 16kHz
                        st.session_state["debug_message"] = "Processing audio chunk..."
                        # print("Processing audio chunk... (in thread)")

                        # Concatenate all collected frames into a single numpy array
                        concatenated_audio = np.concatenate(audio_buffer_list).astype(np.int16)
                        # Create AudioData object for SpeechRecognition
                        audio_data = sr.AudioData(concatenated_audio.tobytes(), SAMPLE_RATE, 2) # 2 bytes per sample (int16)

                        audio_buffer_list = [] # Clear buffer after processing
                        
                        st.session_state["debug_message"] = f"Audio chunk collected. Size: {len(concatenated_audio)} samples."
                        # print(f"Audio chunk collected. Size: {len(concatenated_audio)} samples. (in thread)")

                        try:
                            st.session_state["debug_message"] = "Attempting speech recognition..."
                            # print("Attempting speech recognition... (in thread)")
                            query = recognizer.recognize_google(audio_data, language="en-US").lower()
                            st.session_state["debug_message"] = f"Recognized (in thread): '{query}'"
                            # print(f"Recognized (in thread): '{query}'") # For terminal debugging

                            if WAKE_WORD in query:
                                # Wake word detected! Extract the actual query.
                                st.session_state["debug_message"] = "Wake word detected! " + SPEAKER_NAME + " is processing your request."
                                # print(f"Wake word detected! {SPEAKER_NAME} is processing your request. (in thread)")
                                
                                # Split the query to get content after the wake word
                                actual_query = query.split(WAKE_WORD, 1)[1].strip()
                                
                                if actual_query:
                                    st.session_state["new_query"] = actual_query
                                    st.session_state["new_answer"] = f"You said: '{actual_query}' (This is a placeholder answer)"
                                else:
                                    st.session_state["new_query"] = WAKE_WORD # Only wake word was said
                                    st.session_state["new_answer"] = f"Yes? How can I help you? (Only wake word detected)"

                                # Trigger a Streamlit rerun to update UI
                                st.rerun() 
                                
                                # Clear the audio buffer to avoid processing old audio again
                                audio_buffer_list = [] 
                                st.session_state["debug_message"] = "Response generated and UI updated."

                            else:
                                st.session_state["debug_message"] = f"'{query}' (no wake word). Still listening..."
                                # print(f"'{query}' (no wake word). Still listening... (in thread)")

                        except sr.UnknownValueError:
                            st.session_state["debug_message"] = "Speech Recognition could not understand audio."
                            # print("Speech Recognition could not understand audio (in thread)")
                        except sr.RequestError as e:
                            st.session_state["debug_message"] = f"Google SR service error: {e}"
                            # print(f"Could not request results from Google Speech Recognition service (in thread); {e}")
                        except Exception as e:
                            st.session_state["debug_message"] = f"An unexpected error occurred during audio processing: {e}"
                            # print(f"An unexpected error occurred during audio processing in thread: {e}")
                else:
                    st.session_state["debug_message"] = "Waiting for more audio..."
                    # print("Waiting for more audio... (in thread)")
                    time.sleep(0.01) # Small sleep to prevent busy-waiting

        except Exception as e:
            st.session_state["debug_message"] = f"FATAL ERROR in listen_and_process_thread: {e}"
            # print(f"FATAL ERROR in listen_and_process_thread: {e}")
            listening_active_event.clear() # Clear event to stop thread on fatal error
            st.rerun() # Trigger rerun to show error


# --- Custom Audio Processor for Streamlit-WebRTC ---
# This class needs to inherit from AudioProcessorBase
class AudioProcessor(AudioProcessorBase):
    def recv(self, frame: np.ndarray) -> None:
        # frame is a numpy array (samples, channels)
        # Ensure it's mono and int16 as expected by SpeechRecognition
        if frame.ndim > 1 and frame.shape[1] > 1:
            frame = frame.mean(axis=1) # Convert to mono by averaging channels
        
        if frame.dtype != np.int16:
            frame = frame.astype(np.int16)

        try:
            # Put the numpy array directly into the queue
            st.session_state["audio_queue"].put_nowait(frame)
        except queue.Full:
            # If the queue is full, the processing thread is not keeping up.
            # We can drop frames to avoid blocking the WebRTC stream.
            pass


# --- Streamlit UI ---
st.set_page_config(layout="wide")
st.title("üó£Ô∏è Voice AI Assistant")

debug_placeholder = st.empty() # Placeholder for real-time debug messages

# Streamlit-WebRTC component
webrtc_ctx = webrtc_streamer(
    key="voice_input", # Unique key for the streamer
    mode=WebRtcMode.SENDONLY, # We only send audio from browser to server
    audio_processor_factory=AudioProcessor,
    media_stream_constraints={"audio": True, "video": False}, # Only request audio
    rtc_configuration={
        "iceServers": [
            {"urls": ["stun:stun.l.google.com:19302"]},
            # Add a public TURN server for testing
            {"urls": ["turn:openrelay.metered.ca:80"], "username": "openrelayproject", "credential": "openrelayproject"}
        ]
    },
)

# Start/Stop Listening button logic
# This logic triggers when the WebRTC connection state changes
if webrtc_ctx.state.playing and not st.session_state["listening_active"]:
    st.session_state["listening_active"] = True
    st.session_state["listen_event"].set() # Set the event to start the thread processing
    st.session_state["debug_message"] = "WebRTC connected. Listening active."
    st.experimental_rerun() # Rerun to update UI message and button state
elif not webrtc_ctx.state.playing and st.session_state["listening_active"]:
    st.session_state["listening_active"] = False
    st.session_state["listen_event"].clear() # Clear the event to stop the thread processing
    # Optionally, clear the queue when stopping to prevent old audio from being processed later
    while not st.session_state["audio_queue"].empty():
        try:
            st.session_state["audio_queue"].get_nowait()
        except queue.Empty:
            pass
    st.session_state["debug_message"] = "WebRTC disconnected. Listening stopped."
    st.experimental_rerun() # Rerun to update UI message and button state
elif not st.session_state["listening_active"]:
    st.session_state["debug_message"] = "Click 'Start' button above to activate microphone."


# Start the listening thread only once when the app initializes
if not st.session_state["listen_thread_started"]:
    st.session_state["listen_thread"] = threading.Thread(
        target=listen_and_process_thread,
        args=(st.session_state["listen_event"], st.session_state["audio_queue"]),
        daemon=True # Daemon threads exit automatically when the main program exits
    )
    st.session_state["listen_thread"].start()
    st.session_state["listen_thread_started"] = True
    st.session_state["debug_message"] = "Background listening thread initialized."


# Display debug messages
debug_placeholder.info(st.session_state["debug_message"])

# Display query and answer if available
if st.session_state["new_query"]:
    st.subheader("Your Query:")
    st.write(st.session_state["new_query"])
    st.subheader("AI Assistant's Response:")
    st.write(st.session_state["new_answer"])

# Optional: Clear results if listening stops or on button click
if st.button("Clear Results"):
    st.session_state["new_query"] = None
    st.session_state["new_answer"] = None
    st.session_state["debug_message"] = "Results cleared."
    st.experimental_rerun()
