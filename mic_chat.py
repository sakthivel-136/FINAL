# -*- coding: utf-8 -*-
"""mic_chat

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Ei3nfgH_cdVVbbmd-_EQurgt7s4rvQs
"""

import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode
import av
import queue
import threading
import time
import tempfile
from gtts import gTTS
from scipy.io.wavfile import write
import numpy as np
import pandas as pd
import os
import pickle
import speech_recognition as sr
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Constants
CSV_FILE = "kcet.csv"
VECTOR_FILE = "vectorized.pkl"
THRESHOLD = 0.8
WAKE_WORD = "hey kcet"

# Load or vectorize CSV data
def load_or_vectorize():
    if os.path.exists(VECTOR_FILE):
        with open(VECTOR_FILE, "rb") as f:
            vectorizer, vectors, df = pickle.load(f)
    else:
        df = pd.read_csv(CSV_FILE)
        df['Question'] = df['Question'].str.strip().str.lower()
        vectorizer = TfidfVectorizer()
        vectors = vectorizer.fit_transform(df['Question'])
        with open(VECTOR_FILE, "wb") as f:
            pickle.dump((vectorizer, vectors, df), f)
    return vectorizer, vectors, df

vectorizer, vectors, df = load_or_vectorize()
audio_queue = queue.Queue()

st.set_page_config(page_title="üéôÔ∏è KCET Voice Assistant", layout="centered")
st.markdown("""
    <style>
    .bot-bubble {
        background-color: #e0f7fa;
        color: #006064;
        padding: 1em;
        border-radius: 12px;
        margin-bottom: 10px;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .user-bubble {
        background-color: #fce4ec;
        color: #880e4f;
        padding: 1em;
        border-radius: 12px;
        margin-bottom: 10px;
        text-align: right;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .typing {
        font-style: italic;
        color: #9e9e9e;
        animation: pulse 1s infinite;
    }
    @keyframes pulse {
        0% { opacity: 0.3; }
        50% { opacity: 1; }
        100% { opacity: 0.3; }
    }
    </style>
""", unsafe_allow_html=True)

st.title("üéôÔ∏è KCET Voice Assistant")
status = st.empty()
transcript_placeholder = st.empty()
bot_response = st.empty()
history_placeholder = st.container()

chat_history = []

class AudioProcessor:
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        audio = frame.to_ndarray()
        audio_queue.put_nowait(audio)
        return frame

def listen_and_process():
    recognizer = sr.Recognizer()

    while True:
        if audio_queue.qsize() > 150:
            audio_data = np.concatenate(list(audio_queue.queue), axis=0).astype(np.int16)
            audio_queue.queue.clear()

            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_wav:
                write(temp_wav.name, 16000, audio_data)
                with sr.AudioFile(temp_wav.name) as source:
                    try:
                        audio = recognizer.record(source)
                        query = recognizer.recognize_google(audio).lower()
                    except Exception:
                        continue

            if WAKE_WORD in query:
                while audio_queue.qsize() < 150:
                    time.sleep(0.2)

                audio_data = np.concatenate(list(audio_queue.queue), axis=0).astype(np.int16)
                audio_queue.queue.clear()
                with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_wav:
                    write(temp_wav.name, 16000, audio_data)
                    with sr.AudioFile(temp_wav.name) as source:
                        try:
                            audio = recognizer.record(source)
                            query = recognizer.recognize_google(audio).lower()
                        except Exception:
                            continue

                query_vector = vectorizer.transform([query])
                similarity = cosine_similarity(query_vector, vectors)
                max_sim = similarity.max()
                max_index = similarity.argmax()

                if max_sim >= THRESHOLD:
                    answer = df.iloc[max_index]['Answer']
                else:
                    answer = "ü§ñ I couldn't understand that. Please ask again."

                st.session_state["new_query"] = query
                st.session_state["new_answer"] = answer

webrtc_streamer(
    key="voice",
    mode=WebRtcMode.SENDONLY,
    audio_processor_factory=lambda: AudioProcessor(),
    media_stream_constraints={"audio": True, "video": False},
)

threading.Thread(target=listen_and_process, daemon=True).start()

if "new_query" in st.session_state and "new_answer" in st.session_state:
    user = st.session_state.pop("new_query")
    answer = st.session_state.pop("new_answer")

    chat_history.append((user, answer))

    transcript_placeholder.markdown(f"<div class='user-bubble'>üë§ {user}</div>", unsafe_allow_html=True)
    bot_response.markdown(f"<div class='bot-bubble typing'>ü§ñ Thinking...</div>", unsafe_allow_html=True)
    time.sleep(1)
    bot_response.markdown(f"<div class='bot-bubble'>ü§ñ {answer}</div>", unsafe_allow_html=True)

    tts = gTTS(answer)
    tts_fp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts.save(tts_fp.name)
    st.audio(tts_fp.name, format="audio/mp3")

    with history_placeholder:
        st.markdown("## üìú Chat History")
        for user, bot in reversed(chat_history):
            st.markdown(f"<div class='user-bubble'>üë§ {user}</div>", unsafe_allow_html=True)
            st.markdown(f"<div class='bot-bubble'>ü§ñ {bot}</div>", unsafe_allow_html=True)
            st.markdown("<br>", unsafe_allow_html=True)
