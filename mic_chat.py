# -*- coding: utf-8 -*-
"""mic_chat

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Ei3nfgH_cdVVbbmd-_EQurgt7s4rvQs
"""

import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, VideoProcessorBase, ClientSettings
from streamlit_webrtc import WebRtcStreamerContext, AudioProcessorBase
import numpy as np
import threading
import queue
import speech_recognition as sr
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# --- Configuration ---
WAKE_WORD = "hey kcet"
SPEAKER_NAME = "kcet" # Replace with your AI assistant's name
THRESHOLD = 0.75 # Similarity threshold for wake word
AUDIO_CHUNK_SIZE = 1024  # Size of audio chunks for processing
AUDIO_FORMAT = sr.AudioFile  # Format for SpeechRecognition
SAMPLE_RATE = 16000  # Sample rate for audio (common for speech)

# --- Session State Initialization ---
if "listening_active" not in st.session_state:
    st.session_state["listening_active"] = False
if "new_query" not in st.session_state:
    st.session_state["new_query"] = None
if "new_answer" not in st.session_state:
    st.session_state["new_answer"] = None
if "debug_message" not in st.session_state:
    st.session_state["debug_message"] = "Initializing..."

# --- Audio Queue for Inter-Thread Communication ---
audio_q = queue.Queue()

# --- Speech Recognition and Processing Thread ---
def listen_and_process_thread(listening_active_event: threading.Event):
    recognizer = sr.Recognizer()
    global audio_q
    st.session_state["debug_message"] = "Thread started, awaiting activation."

    while True:
        listening_active_event.wait() # Wait until event is set (Streamlit button clicked)

        st.session_state["debug_message"] = "üëÇ Listening for '" + WAKE_WORD + "'"
        # print("üëÇ Listening for '" + WAKE_WORD + "' (in thread)") # For terminal debugging

        audio_buffer = [] # Buffer to collect audio for recognition

        try:
            while listening_active_event.is_set():
                if not audio_q.empty():
                    frame = audio_q.get()
                    audio_buffer.append(frame.to_ndarray().flatten())
                    # print(f"AudioProcessor: Queue size = {audio_q.qsize()}") # For terminal debugging
                    st.session_state["debug_message"] = f"AudioProcessor: Queue size = {audio_q.qsize()}"

                    # Process accumulated audio after a certain buffer size (e.g., 2 seconds of audio)
                    # 16000 samples/sec * 2 bytes/sample * 2 seconds = 64000 bytes.
                    # Each frame is AUDIO_CHUNK_SIZE * 2 bytes (stereo) or AUDIO_CHUNK_SIZE * 1 byte (mono)
                    # Assuming mono, 16000 samples / AUDIO_CHUNK_SIZE = chunks per second.
                    # For 2 seconds, need 2 * (16000 / AUDIO_CHUNK_SIZE) chunks.
                    if len(audio_buffer) * AUDIO_CHUNK_SIZE >= SAMPLE_RATE * 2: # ~2 seconds of audio
                        st.session_state["debug_message"] = "Processing audio chunk..."
                        # print("Processing audio chunk... (in thread)")

                        concatenated_audio = np.concatenate(audio_buffer).astype(np.int16)
                        audio_data = sr.AudioData(concatenated_audio.tobytes(), SAMPLE_RATE, 2) # 2 bytes per sample (int16)

                        audio_buffer = [] # Clear buffer after processing
                        
                        st.session_state["debug_message"] = f"Audio chunk collected. Size: {len(concatenated_audio)} samples."
                        # print(f"Audio chunk collected. Size: {len(concatenated_audio)} samples. (in thread)")

                        try:
                            st.session_state["debug_message"] = "Attempting speech recognition..."
                            # print("Attempting speech recognition... (in thread)")
                            query = recognizer.recognize_google(audio_data, language="en-US").lower()
                            st.session_state["debug_message"] = f"Recognized (in thread): '{query}'"
                            # print(f"Recognized (in thread): '{query}'") # For terminal debugging

                            if WAKE_WORD in query:
                                # Wake word detected! Extract the actual query.
                                st.session_state["debug_message"] = "Wake word detected! " + SPEAKER_NAME + " is processing your request."
                                # print(f"Wake word detected! {SPEAKER_NAME} is processing your request. (in thread)")
                                actual_query = query.split(WAKE_WORD, 1)[1].strip()
                                if actual_query:
                                    st.session_state["new_query"] = actual_query
                                    st.session_state["new_answer"] = f"You said: '{actual_query}' (This is a placeholder answer)"
                                else:
                                    st.session_state["new_query"] = WAKE_WORD # Only wake word was said
                                    st.session_state["new_answer"] = f"Yes? How can I help you? (Only wake word detected)"

                                # Stop listening after wake word and query
                                listening_active_event.clear() # Signal to stop
                                st.rerun() # Trigger a Streamlit rerun to update UI
                                break # Exit inner while loop to restart listening
                            else:
                                st.session_state["debug_message"] = "No wake word. Still listening..."
                                # print("No wake word. Still listening... (in thread)")

                        except sr.UnknownValueError:
                            st.session_state["debug_message"] = "Speech Recognition could not understand audio (in thread)"
                            # print("Speech Recognition could not understand audio (in thread)")
                        except sr.RequestError as e:
                            st.session_state["debug_message"] = f"Could not request results from Google Speech Recognition service (in thread); {e}"
                            # print(f"Could not request results from Google Speech Recognition service (in thread); {e}")
                else:
                    st.session_state["debug_message"] = "Waiting for more audio..."
                    # print("Waiting for more audio... (in thread)")
                    # Small sleep to prevent busy-waiting if queue is empty
                    import time
                    time.sleep(0.01)

            # If listening_active_event is cleared, the thread will loop back and wait
            if not listening_active_event.is_set():
                st.session_state["debug_message"] = "Listening paused. Click 'Start Listening' to resume."
                # print("Listening paused. (in thread)")


        except Exception as e:
            st.session_state["debug_message"] = f"An error occurred in listen_and_process_thread: {e}"
            # print(f"An error occurred in listen_and_process_thread: {e}")
            listening_active_event.clear() # Ensure event is cleared on error
            st.rerun() # Trigger rerun to show error

# --- Custom Audio Processor for Streamlit-WebRTC ---
class AudioProcessor(AudioProcessorBase):
    def __init__(self) -> None:
        self.audio_data_queue = audio_q
        self.start_time = None

    def recv(self, frame: np.ndarray) -> None:
        # frame is a numpy array of shape (samples, channels)
        # Convert to mono if stereo
        if frame.ndim > 1 and frame.shape[1] > 1:
            frame = frame.mean(axis=1) # Convert to mono by averaging channels
        
        # Ensure the frame is int16 if it's not already
        if frame.dtype != np.int16:
            frame = frame.astype(np.int16)

        try:
            self.audio_data_queue.put(frame, block=False)
        except queue.Full:
            # st.session_state["debug_message"] = "Audio queue is full! Dropping frames."
            pass # Silently drop frames if queue is full to avoid blocking

# --- Streamlit UI ---
st.set_page_config(layout="wide")
st.title("üó£Ô∏è Voice AI Assistant")

debug_placeholder = st.empty() # Placeholder for real-time debug messages

# Start / Stop Listening button
col1, col2 = st.columns([1, 4])
with col1:
    if st.button("Start Listening") and not st.session_state["listening_active"]:
        st.session_state["listening_active"] = True
        st.session_state["debug_message"] = "Starting listening..."
        # Trigger the thread to start processing if it's waiting
        if "listen_event" in st.session_state:
            st.session_state["listen_event"].set()
        st.experimental_rerun() # Rerun to update UI immediately
    elif st.button("Stop Listening") and st.session_state["listening_active"]:
        st.session_state["listening_active"] = False
        st.session_state["debug_message"] = "Listening stopped."
        # Signal the thread to stop processing
        if "listen_event" in st.session_state:
            st.session_state["listen_event"].clear()
        st.experimental_rerun() # Rerun to update UI immediately

# WebRTC Streamer
webrtc_ctx = webrtc_streamer(
    key="voice",
    mode=WebRtcMode.SENDONLY, # We only send audio from browser to server
    audio_processor_factory=AudioProcessor,
    media_stream_constraints={"audio": True, "video": False}, # Only request audio
    rtc_configuration={
        "iceServers": [
            {"urls": ["stun:stun.l.google.com:19302"]},
            # Add a public TURN server for testing
            {"urls": ["turn:openrelay.metered.ca:80"], "username": "openrelayproject", "credential": "openrelayproject"}
        ]
    },
    client_settings=ClientSettings(
        rtc_configuration={
            "iceServers": [
                {"urls": ["stun:stun.l.google.com:19302"]},
                # Add a public TURN server for testing (ensure consistent with server-side)
                {"urls": ["turn:openrelay.metered.ca:80"], "username": "openrelayproject", "credential": "openrelayproject"}
            ]
        }
    )
)

# Start the listening thread only once
if "listen_thread_started" not in st.session_state:
    st.session_state["listen_event"] = threading.Event()
    st.session_state["listen_thread"] = threading.Thread(
        target=listen_and_process_thread,
        args=(st.session_state["listen_event"],),
        daemon=True
    )
    st.session_state["listen_thread"].start()
    st.session_state["listen_thread_started"] = True
    st.session_state["debug_message"] = "Background listening thread initialized."

# Display debug messages
debug_placeholder.info(st.session_state["debug_message"])

# Display query and answer if available
if st.session_state["new_query"]:
    st.subheader("Your Query:")
    st.write(st.session_state["new_query"])
    st.subheader("AI Assistant's Response:")
    st.write(st.session_state["new_answer"])

# Optional: Clear results if listening stops or on button click
if st.button("Clear Results"):
    st.session_state["new_query"] = None
    st.session_state["new_answer"] = None
    st.experimental_rerun()
